For the one-dimensional simulations explored thus far, computational cost is not of particular concern. Such numerical experiments can be evaluated using laptop or desktop computers without much attention to memory and compute scalability. Exercises in hyper-reduction for PROMs of such small dimension is largely academic, as even the cost of the full-order model measures in core-seconds or core-minutes. For more practical two- and three-dimensional systems, however, the computational cost of evaluating the FOM and unsampled PROMs is significantly larger, requiring high-performance computing resources and parallelism across hundreds or thousands of cores. The benefit of hyper-reduction in enabling scalable computations for PROMs is more readily apparent for such high-dimensional non-linear systems.

In this chapter, HPROMs are analyzed for two multi-scale systems: transonic flow over an open cavity and a single-element model rocket combustor. The cavity flow system acts as a proof-of-concept for non-reacting flows, while the rocket combustor more deeply explores the challenges in developing robust and scalable HPROMs for reacting flows. Critically, the performance of several gappy POD sampling algorithms are compared, evaluating both the offline cost of the sample selection algorithms and their effects on memory consumption, online computational cost, and accuracy.

Throughout this chapter, results are presented on computational cost savings induced by HPROMs as a \textit{speedup ratio} in terms of core-hour consumption (\textit{core time}). Core time is computed by multiplying the amount of wall time spent running the simulation by the number of CPU cores used to compute the simulation. Core time metrics provide a more complete measurement of computational cost, though wall time measurements may be useful in assessing performance for time-sensitive applications. The speedup ratio is computed as the ratio of the FOM core time to the PROM core time measurement. For example, if a FOM calculation using ten cores took five hours, and a PROM calculation using two cores took one hour, the core time speedup ratio would be 25. Note that run-time measurements are computed as the \textit{average} time across all processes spent performing significant floating-point operations (\textit{calculation time}) and communicating data between processes via MPI (\textit{MPI time}). I/O timing (e.g., writing field or probe data to disk) is excluded, as it can be a volatile and unreliable measure, depending on hardware limitations and file system usage. There is some inaccuracy in comparing run-times and computational cost from calculation and MPI timings alone, as the overall computing time is not equal to the average across all parallel processes. Nor is it the sum of the maximum calculation time and maximum MPI time, as the process which computes the most floating-point operations may not be the same process which communicates the most data (or waits idly for other processes). However, this approach is as fair a measure as can be reasonably be expected.

All simulations are computed on the Cray XC40/50 Onyx cluster maintained by the U.S. Army Engineering Research and Development Center. A single compute node features two Intel E5-2699v4 Broadwell chips (2.8 GHz), each with 22 computational cores for a total of 44 cores per node. Nodes have 121 GB of accessible memory, and are connected by Cray Aries interconnects.
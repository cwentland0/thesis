\section{Accelerating ROMs of Non-linear Systems}\label{sec:hyperreduction}

So far, we have pointedly ignored the fact that the Galerkin (Eq.~\ref{eq:galerkinROMODE}), LSPG (Eq.~\ref{eq:lspgLSLin}), and MP-LSVT (Eq.~\ref{eq:mplsvtLSLin}) ROMs \textit{will not generate computational cost savings} for large-scale non-linear systems. This is largely due to the fact that evaluating the non-linear terms $\rhsFunc{\cdot}$ arising from fluxes, source terms, body forces, and boundary conditions still scales with the full-order dimension $\numDOF$. Linear time-invariant systems do not suffer from this issue; given a system of the form $\consVecDt - \dummyMat \consVec = \zeroVec$, $\dummyMat \inRTwo{\numDOF}{\numDOF}$, the resulting Galerkin ROM (assuming $\consScale = \resScale = \identMat$) takes the form
%
\begin{equation}
    \ode{\consVecCoef}{\timeVar} = \consTrial^\top \dummyMat \consTrial \consVecCoef.
\end{equation}
%
The matrix $\consTrial^\top \dummyMat \consTrial \consVecCoef \inRTwo{\numConsModes}{\numConsModes}$ can be precomputed in the \textit{offline} stage before evaluating the ROM in the \textit{online} stage. Such a precomputation is impossible for general non-linear systems. The low-dimensional state must first be \textit{lifted} to the full-dimensional state (via $\decoderFunc{\consVecCoef}$) to evaluate the non-linear terms $\rhsFunc{\cdot}$. In the case of Galerkin projection, this term must then be projected onto the tangent trial space before integrating the low-dimensional ODE in time. In the case of LSPG and MP-LSVT ROMs, the time-variant test basis must be computed from the residual Jacobian. Despite the fact that the resulting low-dimensional system may be less expensive to temporally integrate, these additional operations often outweigh any cost savings. Particularly for complex multi-physics systems, the evaluation of $\rhsFunc{\cdot}$ accounts for the vast majority of the solver cost, and failing to reduce this cost often fails to reduce the ROM cost below that of the FOM. 

To achieve the intended goal of significantly reducing the cost of evaluating the model, we must eliminate the ROM's dependence on the full-order dimension $\numDOF$. Techniques which seek this goal are generally referred to as \textit{hyper-reduction} methods. Most prevalent and mature are so-called \textit{sparse sampling} methods, which evaluate the non-linear terms at a small number of carefully-selected degrees of freedom and reconstruct them approximately using data-driven modeling techniques. Work in this thesis focuses on linear sparse-sampling methods for linear subspace ROMs, namely the discrete empirical interpolation method and gappy proper orthogonal decomposition. Neural network approaches to hyper-reduction~\cite{nnHyperRed} and hyper-reduction for non-linear manifold ROMs~\cite{Kim2022} have been proposed, but are not discussed or analyzed here.

\subsection{DEIM and Gappy POD}

Two well-established hyper-reduction methods for projection-based ROMs of discrete dynamical systems are the discrete empirical interpolation method (DEIM) and gappy proper orthogonal decomposition (gappy POD). The two are closely related, but were originally developed for different applications. DEIM, introduced by Chaturantabut and Sorensen~\cite{Chaturantabut2010}, is a discrete formulation of the empirical interpolation method (EIM)~\cite{Barrault2004}. It introduces an approximation of non-linear functions of the form,

\begin{equation}\label{eq:deimRHSApprox}
    \resFunc{\consVec, \; \timeVar} \approx \resApproxFunc{\consVec, \; \timeVar} = \deimBasis \big[ \sampMat^T \deimBasis \big]^{-1} \sampMat^T \resFunc{\consVec, \; \timeVar},
\end{equation}
where $\sampMat = [\canonVec_{\dummyIdx_1}, \; \canonVec_{\dummyIdx_2}, \; \hdots, \; \canonVec_{\dummyIdx_{\numSamps}} ] \inRTwo{\numDOF}{\numSamps}$ is composed of \numSamps\ unique canonical unit vectors $\canonVec_{\dummyIdx} \inROne{\numDOF}$. The operation $\sampMat^T \resVec \inROne{\numSamps}$ thus selects each $\dummyIdx$th degree of freedom from $\resVec$. The matrix $\deimBasis = [\deimBasisVec_1, \; \deimBasisVec_2, \; \hdots, \; \deimBasisVec{\numResModes}] \inRTwo{\numDOF}{\numResModes}$ is an orthonormal basis, usually generated by POD from FOM snapshots of the non-linear function $\resVec$. It is assumed that the matrix $\sampMat^T \deimBasis \inRTwo{\numSamps}{\numResModes}$ has full rank. In computing $\resApproxVec$, we see that the non-linear function $\resVec$ must only be computed, or \textit{sampled}, at $\numSamps$ degrees of freedom, instead of its full dimension $\numDOF$. When $\numSamps \ll \numDOF$, the cost of computing $\resApproxVec$ may be much less than the cost of computing $\resVec$.

Note that here, the number of DEIM basis modes is equal to the number of sampled degrees of freedom, i.e., $\numSamps = \numResModes$. By this formulation, the non-linear function is interpolated exactly at $\numSamps < \numDOF$ degrees of freedom and interpolated approximately at all other degrees of freedom. The error in this approximation is bounded~\cite{Chaturantabut2010} by the inequality

\begin{equation}\label{eq:deimErrorBound}
   \left\Vert \resFunc{\consVec, \; \timeVar} - \resApproxFunc{\consVec, \; \timeVar} \right\Vert_2 \le \left\Vert \left[ \sampMat^T \deimBasis \right]^{-1} \right\Vert_2 \left\Vert \left[ \identMat - \deimBasis \deimBasis^T \right] \resFunc{\consVec, \; \timeVar} \right\Vert_2,
\end{equation}
where the first norm term on the right-hand side is a measure of the \textit{sampling error}, and the second norm term is the \textit{projection error}. As will be discussed shortly, various methods of selecting the sampling degrees of freedom often attempt to minimize these sources of error.

DEIM has been successfully applied to a variety of interesting dynamical systems~\cite{Chaturantabut2011, Henneron2014}, but is quite limited by the restriction $\numSamps = \numResModes$. Recall that for practical engineering systems, $\numResModes \ll \numDOF$ generally, and such extreme sparse sampling may result in high interpolation error at the unsampled degrees of freedom. Furthermore, Peherstorfer et al.~\cite{Peherstorfer2020} show that increasing $\numResModes$ can lead to an unstable increase in the interpolation error.

Gappy POD, originally developed well before the advent of DEIM to approximate full field data from a few sparse samples~\cite{Everson1995}, can also be used to approximate non-linear functions in dynamical systems. The method takes a very similar approach to DEIM, but relaxes the restriction on the number of sampled degrees of freedom, allowing $\numSamps > \numResModes$. Thus, the approximation becomes a least-squares regression of the form,
\begin{equation}\label{eq:gappyPODRHSApprox}
    \resFunc{\consVec, \; \timeVar} \approx \resApproxFunc{\consVec, \; \timeVar} = \deimBasis \big[ \sampMat^T \deimBasis \big]^{+} \sampMat^T \resFunc{\consVec, \; \timeVar},
\end{equation}
where the operation $\dummyMat^+$ indicates the Moore--Penrose inverse (or pseudo-inverse). Again, this assumes $\sampMat^T \deimBasis$ has full rank. Similarly to Eq.~\ref{eq:deimErrorBound}, the gappy POD regression error is bounded by

% \begin{equation}\label{eq:gappyPODErrorBound}
%     \Big| \Big|\resFunc{\consVec, \; \timeVar} - \resFuncDEIM{\consVec, \; \timeVar} \Big| \Big|_2 \le \Big| \Big| \big[ \sampMat^T \deimBasis \big]^{+} \Big| \Big|_2 \Big| \Big|\big[ \identMat - \deimBasis \deimBasis^T \big] \resFunc{\consVec, \; \timeVar} \Big| \Big|_2.
% \end{equation}

\subsection{Sample Selection}
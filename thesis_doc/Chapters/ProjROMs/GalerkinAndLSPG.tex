\section{Classical Projection-based ROMs}\label{sec:classicROMs}

Inserting the approximation $\consVec \approx \consVecRom = \decoderFunc{\consVecCoef}$ into the governing ODE (Eq.~\ref{eq:FOM}) results in the system
%
\begin{equation}\label{eq:romFullODE}
    \ode{\decoderFunc{\consVecCoef}}{\timeVar} - \rhsFunc{\consVecRom, \; \timeVar} = \zeroVec, \quad \consVecRom^0 = \consFuncRom{\initTime}.
\end{equation}
%
Employing the chain rule, and scaling the ODE via the diagonal matrix $\resScaleInv \inRTwo{\numDOF}{\numDOF}$ to standardize the semi-discrete residual, this system can be modified into
%
\begin{equation}\label{eq:romFullODEMod}
    \resScaleInv \left[ \jacobDecodeCons \ode{\consVecCoef}{\timeVar} - \rhsFunc{\consVecRom, \; \timeVar} \right] = \zeroVec,
\end{equation}
%
where the Jacobian $\jacobDecodeCons \defEq \funcMap{\partial \decoderFunc{\consVecCoef} / \partial \consVecCoef}{\ROne{\numConsModes}}{\RTwo{\numDOF}{\numConsModes}}$. For a linear representation, the Jacobian is simply $\jacobDecodeCons = \consScale \consTrial$. The purpose of the residual scaling will be made apparent later.

This has obviously resulted in no practical dimension reduction, as Eqs~\ref{eq:romFullODE} and~\ref{eq:romFullODEMod} are still $\numDOF$-dimensional systems. To accomplish dimension reduction, we require a \textit{projection} operation, the namesake of projection-based reduced-order models. This entails choosing a \textit{test basis} $\testBasisCons \inRTwo{\numDOF}{\numConsModes}$ by which the approximate ODE is projected as
%
\begin{equation}\label{eq:contRomRes}
    \testBasisCons^\top \resScaleInv \left[\jacobDecodeCons \ode{\consVecCoef}{\timeVar} - \rhsFunc{\consVecRom, \; \timeVar} \right] = \zeroVec.
\end{equation}
%
Assuming $\testBasisCons^\top \resScaleInv \jacobDecodeCons \inRTwo{\numConsModes}{\numConsModes}$ is invertible, rearranging terms arrives at
%
\begin{equation}\label{eq:projRomGen}
    \ode{\consVecCoef}{\timeVar} = \left[\testBasisCons^\top \resScaleInv \jacobDecodeCons\right]^{-1} \testBasisCons^\top \resScaleInv \rhsFunc{\consVecRom, \; \timeVar}.
\end{equation}
%
Finally, Eq.~\ref{eq:projRomGen} is a $\numConsModes$-dimensional ODE which can be marched forward in time with respect to the latent state $\consVecCoef$. As we naturally choose $\numConsModes \ll \numDOF$, this time integration process is theoretically cheaper than that of the FOM. This may not always be the case above a certain value of $\numConsModes$, as the inversion (and reduction across processes for parallel applications) of the dense matrix $\testBasisCons^\top \resScaleInv \jacobDecodeCons$ may be more computationally expensive than the solution of the FOM's sparse linear system arising from Eq.~\ref{eq:pseudoTimeLinSys}. Further, the evaluation of the non-linear terms $\rhsFunc{\cdot}$ still depends on the full-order state $\consVecRom$. These and other computational barriers will be addressed in more detail in Chapter~\ref{chap:HPROMS}.

The question now becomes how the projecting test basis is chosen. In the following sections, we detail the classical projection methods, namely Galerkin projection and least-squares Petrov--Galerkin projection. In Section~\ref{sec:mplsvt} we will derive the recent model-form preserving least-squares with variable transformation method and discuss its potential advantages over classical projection methods.

\subsection{Galerkin Projection}

Galerkin projection can be defined as the projection of the full-order ROM ODE onto the tangent space of the trial manifold, i.e.
%
\begin{equation}
    \testBasisCons\left(\consVecCoef\right) \defEq \jacobDecodeConsFunc{\consVecCoef}.
\end{equation}
%
Substituting this into Eq.~\ref{eq:projRomGen} arrives at
%
\begin{equation}\label{eq:galerkinROMODE}
    \ode{\consVecCoef}{\timeVar} = \left[\jacobDecodeCons^\top \resScaleInv \jacobDecodeCons\right]^{-1} \jacobDecodeCons^\top \resScaleInv \rhsFunc{\consVecRom, \; \timeVar}.
\end{equation}
%
As detailed extensively by Lee and Carlberg~\cite{Lee2020}, when employing a non-linear manifold trial space, the solution of Eq.~\ref{eq:galerkinROMODE} with an implicit time integrator via Newton's method requires the calculation of the third-order tensor $\partial \jacobDecodeCons / \partial \consVecCoef$. This is generally computational intractable, and neglecting this term amounts to a quasi-Newton method with an approximate residual Jacobian.

For a linear representation, the test basis is simply $\testBasisCons = \consScale \consTrial$, leading to
%
\begin{equation}\label{eq:galerkinROMODELinear}
	\ode{\consVecCoef}{\timeVar} = \left[\consTrial^\top \consScale \resScaleInv \consScale \consTrial\right]^{-1} \consTrial^\top \consScale \resScaleInv \rhsFunc{\consVecRom, \; \timeVar}.
\end{equation}
%
In the specific case of $\consScale = \resScale = \identMat$, recalling that $\consTrial$ is orthonormal, this simplifies the ODE greatly to the more familiar Galerkin PROM form
%
\begin{equation}\label{eq:galerkinROMODELinearSimple}
    \ode{\consVecCoef}{\timeVar} = \consTrial^\top \rhsFunc{\consVecRom, \; \timeVar}.
\end{equation}

As discussed in Section~\ref{sec:dataDrivenModeling}, Galerkin projection ROMs have a long history of applications to simulations of fluid flows (e.g.,~\cite{Aubry1988,Cazemier1998,BuiThanh2007}). However, they have been observed to exhibit unacceptable stability and accuracy issues for more complex systems. Although the Galerkin PROM can be shown to be continuous-optimal in the sense that it minimizes the $\ell^2$-norm of the semi-discrete ROM residual~\cite{Carlberg2017}, in reality the fully-discrete ROM residual is of more practical concern. Further, there is no expectation that the projection of the non-linear function $\rhsVec$ onto the tangent trial space is accurate, as the conservative state space and the space $\{\rhsFunc{\dummyVec} \vert \; \dummyVec \inROne{\numDOF} \}$ are entirely different. This fact motivates the least-squares Petrov--Galerkin projection method.

\subsection{Least-squares Petrov--Galerkin Projection}

The least-squares Petrov--Galerkin (LSPG) projection method, formulated and refined by Carlberg and coworkers~\cite{Carlberg2010,Carlberg2013,Carlberg2017}, is motivated by minimizing the \textit{fully-discrete ROM residual}, opposed to the semi-discrete residual minimized by Galerkin projection. The fully-discrete ROM residual $\funcMap{\resVec}{\ROne{\numConsModes}}{\ROne{\numDOF}}$ is given as
%
\begin{equation}
    \resFunc{\consVecCoef^\timeIdx} \defEq \consVecRomDt^{\timeIdx} - \rhsFunc{\decoderFunc{\consVecCoef^{\timeIdx}}, \; \timeVar} = \zeroVec,
\end{equation}
%
where $\consVecRomDt$ is the temporal discretization operator (e.g. forward Euler, BDF) of the approximate state $\consVecRom = \decoderFunc{\consVecCoef}$ at the $\timeIdx$th time step. Computing the least-squares minimization of the scaled fully-discrete residual takes the form,
%
\begin{equation}\label{eq:lspgLS}
    \consVecCoef^{\timeIdx} = \argmin{\dummyVec \inROne{\numConsModes}} \left\Vert \resScaleInv \resFunc{\dummyVec} \right\Vert_2,
\end{equation}
%
where we now see the purpose of scaling the residual by $\resScaleInv$. This has the effect of ensuring that, with careful calculation of $\resScale$, the various elements of the residual contribute similarly to the solution of the non-linear least squares problem. This is particularly important for systems which exhibit extreme scale disparities (as is often the case in high-pressure reacting systems) and solvers which cannot readily be non-dimensionalized (such as compressible combustion solvers). The residual weighting is sometimes couched in terms of a weighted norm, written alternatively as
%
\begin{equation}
	\consVecCoef^{\timeIdx} = \argmin{\dummyVec \inROne{\numConsModes}} \left\Vert \resFunc{\dummyVec} \right\Vert_{\resScaleInv},
\end{equation}
%
where the weighted norm indicates $\Vert\resVec\Vert_{\resScaleInv} \defEq \sqrt{\resVec^\top \resScale^{-2} \resVec}$. A detailed exploration of the idea of alternative norms is provided by Parish and Rizzi~\cite{Parish2022}. This concept is also tangentially related to the work by Lindsay \textit{et al.}~\cite{Lindsay2022} on PROM preconditioning, which showed that even a simple Jacobi preconditioner results in drastic improvements in PROM stability and accuracy relative to ROMs of the equivalent unscaled, dimensional system. We discuss this residual weighting process in more detail in Appendix~\ref{sec:resWeight}, but eschew the weighted norm notation for the sake of clarity.

The solution of Eq.~\ref{eq:lspgLS} is usually accomplished iteratively by linearizing the residual at the $\timeIdx$th time step about the solution at the $\newtonIdx$th subiteration, given by the process
%
\begin{equation}\label{eq:lspgLSLin}
    \delta \consVecCoef^{\newtonIdx} = \argmin{\dummyVec \inROne{\numConsModes}} \left\Vert \resScaleInv \left(\jacobResCoefCons^{\newtonIdx} \dummyVec - \resFunc{\consVecCoef^{\newtonIdx}}\right) \right\Vert_2,
\end{equation}
\begin{equation}
    \consVecCoef^{\newtonIdx + 1} = \consVecCoef^{\newtonIdx} + \alpha \left[ \delta \consVecCoef^{\newtonIdx} \right],
\end{equation}
%
where the ROM residual Jacobian is defined as $\jacobResCoefCons^\newtonIdx \defEq \funcMap{\partial \resFunc{\consVecCoef^\newtonIdx} / \partial \consVecCoef}{\ROne{\numConsModes}}{\RTwo{\numDOF}{\numConsModes}}$. The constant factor $\alpha \in \mathbb{R}_+$ is the step size, which is unity for all ROMs in this thesis; alternative methods may adapt $\alpha \defEq \alpha^{\timeIdx,\newtonIdx}$ to control iterative convergence. Further, the least-squares problem may be computed in terms of the full-order Jacobian $\jacobResCons^{\newtonIdx} \defEq \funcMap{\partial \resFunc{\consVecCoef^\newtonIdx} / \partial \consVec}{\ROne{\numConsModes}}{\RTwo{\numDOF}{\numDOF}}$ via the chain rule,
%
\begin{equation}
    \delta \consVecCoef^{\newtonIdx} = \argmin{\dummyVec \inROne{\numConsModes}} \left\Vert \resScaleInv \left[\jacobResCons^{\newtonIdx} \jacobDecodeCons^{\newtonIdx} \dummyVec - \resFunc{\consVecCoef^{\newtonIdx}}\right] \right\Vert_2,
\end{equation}
%
where the decoder Jacobian $\jacobDecodeCons^{\newtonIdx} \defEq \partial \decoderFunc{\consVecCoef^{\newtonIdx}} / \partial \consVecCoef$ is now specified in the time-discrete setting. In the case of a linear trial space, $\jacobDecodeCons^{\newtonIdx} = \consScale \consTrial$, and this simplifies to
%
\begin{equation}
    \delta \consVecCoef^{\newtonIdx} = \argmin{\dummyVec \inROne{\numConsModes}} \left\Vert \resScaleInv \left[\jacobResCons^{\newtonIdx} \consScale \consTrial \dummyVec - \resFunc{\consVecCoef^{\newtonIdx}}\right] \right\Vert_2.
\end{equation}
%
This particular form is useful inasmuch as the full-order residual Jacobian $\jacobResCons^{\newtonIdx}$ is normally already available if a given FOM solver contains implicit time integration capabilities. The form given in Eq.~\ref{eq:lspgLSLin} may be simpler if automatic differentiation tools are available in the solver. In either case, upon sufficient convergence of the solution, the iterative solution is set to the solution at the next physical time step, i.e. $\consVecCoef^{\timeIdx} \leftarrow \consVecCoef^{\newtonIdx}$, and the process is repeated for proceeding time steps.

The simplest means of computing the solution to Eq.~\ref{eq:lspgLSLin} is the formation of the normal equations, given by,
%
\begin{equation}\label{eq:lspgNormal}
    \left[\resScaleInv \jacobResCoefCons^{\newtonIdx}\right]^\top \resScaleInv \jacobResCoefCons^{\newtonIdx} \delta \consVecCoef^{\newtonIdx} = -\left[\resScaleInv \jacobResCoefCons^{\newtonIdx}\right]^\top \resScaleInv \resFunc{\consVecCoef^{\newtonIdx}}.
\end{equation}
%
This formulation reveals the meaning of ``Petrov--Galerkin'' in LSPG: a Petrov--Galerkin projection is any projection in which the test space is \textit{not} the same as the space tangent to the trial manifold (as is the case for Galerkin projection). In Eq.~\ref{eq:lspgNormal}, if we expand terms using the chain rule, we arrive at
%
\begin{equation}
    \left[\resScaleInv \jacobResCons^{\newtonIdx} \jacobDecodeCons^{\newtonIdx}\right]^\top \resScaleInv \jacobResCons^{\newtonIdx} \jacobDecodeCons^{\newtonIdx} \delta \consVecCoef^{\newtonIdx} = -\left[\resScaleInv \jacobResCons^{\newtonIdx} \jacobDecodeCons^{\newtonIdx}\right]^\top \resScaleInv \resFunc{\consVecCoef^{\newtonIdx}}.
\end{equation}
%
This form can be interpreted as a Petrov--Galerkin projection of the Gauss--Newton solution of Eq.~\ref{eq:contRomRes} with a test basis
%
\begin{equation}
    \testBasisCons^\newtonIdx \defEq \resScaleInv \jacobResCons^\newtonIdx \jacobDecodeCons^{\newtonIdx}.
\end{equation}
%
We see here that even with a linear state representation ($\jacobDecodeCons^{\newtonIdx} = \consScale \consTrial$), the test basis is time-variant, unlike Galerkin projection with a linear state representation. This poses a significant computational cost increase over Galerkin projection. For a non-linear state representation, the cost difference is less extreme, as the decoder Jacobian is dependent on the latent state, i.e. $\jacobDecodeCons^{\newtonIdx} \defEq \jacobDecodeConsFunc{\consVecCoef^\newtonIdx}$, and the test basis is time-variant for Galerkin and LSPG projection alike.

Forming the complete normal equations is a computationally expensive process. A simple alternative involves the QR decomposition of the residual Jacobian,
%
\begin{equation}
    \jacobResCoefCons^{\newtonIdx} = \mathbf{QR},
\end{equation}
%
where $\mathbf{Q} \defEq \left[\mathbf{Q}_{\numConsModes}, \; \mathbf{Q}_{\numDOF-\numConsModes}\right] \inRTwo{\numDOF}{\numDOF}$ has orthogonal columns, composed of $\mathbf{Q}_{\numConsModes} \inRTwo{\numDOF}{\numConsModes}$ and $\mathbf{Q}_{\numDOF - \numConsModes} \inRTwo{\numDOF}{\numDOF - \numConsModes}$. The matrix $\mathbf{R} = [\mathbf{R}_{\numConsModes}, \; \zeroVec]^\top \inRTwo{\numDOF}{\numConsModes}$ is composed of upper triangular $\mathbf{R}_{\numConsModes} \inRTwo{\numConsModes}{\numConsModes}$ and a zero matrix $\zeroVec \inRTwo{\numDOF - \numConsModes}{\numConsModes}$. Note that the matrix $\mathbf{R}$ here is not the same as that used for residual scaling, but we temporarily use this notation for the sake of consistency with the literature (and general familiarity with the method). The latent state update is solved from
%
\begin{equation}
    \mathbf{R}_{\numConsModes} \delta \consVecCoef^{\newtonIdx} = \left[\mathbf{Q}_{\numConsModes}\right]^\top \resFunc{\consVecCoef^{\newtonIdx}}.
\end{equation}
%
The upper triangular solve is significantly less expensive than the dense solve in Eq.~\ref{eq:lspgNormal}, and does not require the dense matrix-matrix multiplication $\left[\resScaleInv \jacobResCons^{\newtonIdx}\right]^\top \resScaleInv \jacobResCons^{\newtonIdx}$ (where $\resScaleInv$ is now, again, the residual scaling matrix).

Least-squares Petrov--Galerkin projection has been shown to generate vastly more accurate and stable ROMs compared to Galerkin ROMs. The first publications to outline LSPG computed a robust PROM simulation of a three-dimensional Ahmed body~\cite{Carlberg2010,Carlberg2013}, and later reported excellent results for transonic flow over a two-dimensional open cavity~\cite{Carlberg2017}. Continued work with LSPG has produced stable and accurate ROMs of three-dimensional separated flow over an airfoil at high angle-of-attack~\cite{Grimberg2020Hyper} and flow around a three-dimensional model of an F-16 aircraft~\cite{Grimberg2021}. The success of LSPG has been attributed to the fact that it minimizes the fully-discrete ROM residual~\cite{Grimberg2020} which is, in practice, the set of equations being solved by the PROM. This has the effect of avoiding the inaccurate projection of the non-linear terms $\rhsFunc{\cdot}$ onto the tangent trial space (as seen in Eq.~\ref{eq:galerkinROMODE}), and instead projects the fully-discrete residual via the space tangent to the fully-discrete residual (as seen in Eq.~\ref{eq:lspgNormal}).

However, early applications of both Galerkin and LSPG ROMs to advection-dominated combustion problems indicated that both methods were insufficient in generating robust and accurate ROMs for such systems~\cite{Huang2018b,Huang2019}. Both methods frequently resulted in large deviations in the expected heat release, and often produced non-physical states such as negative temperature or pressure values. Limiter methods were proposed to prevent such non-physical phenomena~\cite{Huang2019,Huang2020}, but are generally considered \textit{ad hoc} solutions. Anecdotally, these issues were linked to persistently stiff ROMs which had difficulty converging due to the high degree of non-linearity induced by chemical source terms, which are extremely sensitive to small changes in chemical composition and temperature.
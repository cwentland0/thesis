\section{Basis and Sampling Adaptation}\label{sec:adaptation}

Up to this point, the trial bases $\consTrial$ and $\primTrial$, regression basis $\deimBasis$, and hyper-reduction samples $\sampMat$ have been treated as constant throughout the duration of the unsteady PROM simulation. Such treatment is normally sufficient to generate stable and accurate ROMs which are strictly \textit{reconstructive}, or those which are computed for the same initial conditions, parameterization, and duration as that of the FOM training simulations. The PROM simply seeks to reconstruct the unsteady solution from which the trial basis and sampling indices were computed. However, the ultimate goal of any data-driven method is to be \textit{predictive}, or provide accurate simulations for initial conditions, parameterizations, or durations which were not accounted for in the training data set. Reduced-order model methods such as balanced truncation or the reduced basis method have been demonstrated to exhibit excellent predictive capabilities for parameterized PDEs, but their success is generally restricted to elliptic or parabolic systems which are characterized by a very smooth solution manifold in parameter space. ROMs of general non-linear, hyperbolic systems, particularly those defined by a linear trial space, often exhibit abysmal predictive performance.

As outlined in Section~\ref{subsec:nonlinManifold}, non-linear trial manifolds have been proposed as a potential solution to this problem. However, as will be discussed in Chapter~\ref{chap:TransientFlame}, the computational cost of training and implementing autoencoder non-linear manifold ROMs, and difficulties in applying effective hyper-reduction, often precludes their useful application. Further, neural networks tend to suffer from \textit{overfitting}, and often perform as poorly or worse than linear approaches in extrapolation in parameter space.

Alternatively, several methods have been proposed which \textit{adapt} a linear trial basis and hyper-reduction sampling configuration during the unsteady solution of the linear subspace PROM. Examples of approaches which propose time-variant trial bases include dictionary-based methods (sometimes referred to as \textit{local} bases)~\cite{Amsallem2012,Peherstorfer2014,Abgrall2016}, space-time trial bases~\cite{Choi2019,Hoang2022}, or basis transport maps~\cite{Iollo2014}. Although these methods have displayed exceptional accuracy improvements over static basis methods for convection-dominated problems, they still suffer from the fact that the constituent bases (e.g., those that form the dictionary) are constructed from the training data and may not accurately represent the unsteady solution when it significantly diverges from that observed in the full-order datasets. A notable exception is the work by Etter and Carlberg~\cite{Etter2019}, which updates the trial basis during online computations by vector space sieving. This thesis explores the AADEIM framework by Peherstorfer~\cite{Peherstorfer2015,Peherstorfer2020Adaptive}, which leverages limited, periodic queries of the full-order model to incorporate the online state evolution in adapting the sampling configuration. This is investigated in combination with the recent one-step adaptation framework by Huang and Duraisamy~\cite{Huang2022a} as a method for adapting the trial basis.

\subsection{AADEIM}

The AADEIM framework, originally posed by Peherstorfer and Willcox~\cite{Peherstorfer2015} and later refined by Peherstorfer~\cite{Peherstorfer2020Adaptive} is outlined here. This section borrows heavily from their notation for the sake of consistency. This method is predicated on the formulation of the fully discrete FOM O$\Delta$E as given in Eq.~\ref{eq:benResidual}, repeated here as
%
\begin{equation}
	\consVec^{\timeIdx-1} - \rhsResFunc{\consVec^{\timeIdx},\; \timeVar^{\timeIdx}} = \zeroVec.
\end{equation}
%
Again, this is an extremely counterintuitive way to write the evolution equations, but has the benefit of framing the residual in such a way as to motivate equating the trial basis $\consScale \consTrial$ and the regression basis $\deimBasis$, as outlined in Section~\ref{subsec:stateApproxDEIM}. From this frame of reference, AADEIM proposes adaptation of the trial basis $\consTrial$ as additive low-rank updates of the form
%
\begin{equation}
	\consTrial^{\timeIdx+1} = \consTrial^{\timeIdx} + \adeimAlpha^{\timeIdx} \left[\adeimBeta^{\timeIdx}\right]^\top.
\end{equation}
%
The matrices $\adeimAlpha \inRTwo{\numDOF}{\numRank}$ and $\adeimBeta \inRTwo{\numConsModes}{\numRank}$ compute the low-rank updated, where $\numRank$ indicates the rank of the update. The update is formulated as the solution to the least-squares minimization of the regression error,
%
\begin{equation}\label{eq:aadeimAlphaBeta}
	\adeimAlpha^{\timeIdx} \left[\adeimBeta^{\timeIdx}\right]^\top = \argmin{\adeimAlpha \inRTwo{\numDOF}{\numRank},\; \adeimBeta \inRTwo{\numConsModes}{\numRank}} \left\Vert \sampMat^{\timeIdx} \left[\left[\consTrial^{\timeIdx} + \adeimAlpha \adeimBeta^\top \right] \left[\sampMat^{\timeIdx} \consTrial^{\timeIdx}\right]^+ \sampMat^{\timeIdx} - \identMat \right] \resWindow^{\timeIdx} \right\Vert_F^2,
\end{equation}
%
where the snapshot matrix $\resWindow^{\timeIdx} \inRTwo{\numDOF}{\numWindow}$ is defined as
%
\begin{align}
	\resWindow^{\timeIdx} &\defEq \left[\rhsResFunc{\consVecRom^{\timeIdx - \numWindow}}, \; \hdots \;, \; \rhsResFunc{\consVecRom^{\timeIdx - 1}}\right] \\
	&\defEq \left[\consVecRom^{\timeIdx - \numWindow + 1}, \; \hdots \;, \; \consVecRom^{\timeIdx} \right]
\end{align}
%
The window size $\numWindow \in \mathbb{N}_0$ represents the number of prior ROM solutions for which the regression error in Eq.~\ref{eq:aadeimAlphaBeta} is considered. That is, during the online solution of the ROM, the state $\consVecRom^{\timeIdx}$ is stored in $\resWindow^{\timeIdx}$, and the solution of least-squares minimization of the regression error of this solution updates the the trial basis $\consTrial$ during the ROM runtime. Note that in the work by Peherstorfer~\cite{Peherstorfer2020Adaptive}, those degrees of freedom which are \textit{not} sampled by $\sampMat$ are simply reconstructed by the gappy POD regression, i.e. $\sampMatComp \consVecRom = \sampMatComp \consTrial^{\timeIdx} \left[\sampMat \consTrial^{\timeIdx}\right]^+ \sampMat \left[\consVecCent + \consScale \consTrial \consVecCoef^{\timeIdx}\right]$, where $\sampMatComp \defEq \{1,\hdots,\numDOF \} \setminus \sampSet, \; \sampMatComp \inRTwo{\numDOF-\numSamps}{\numDOF}$ is the complement of the sample index set.

\begin{algorithm}
	\caption{AADEIM algorithm}\label{alg:aadeim}
	\begin{algorithmic}
		\STATE temp
	\end{algorithmic}
\end{algorithm}

\subsection{One-step Basis Adaptation}
%
The trial space adaptation proposed by Huang \textit{et al.}~\cite{Huang2022a} provides a simpler alternative to that provided by AADEIM. This method begins by suggesting a basis increment $\consTrialUpdate \inRTwo{\numDOF}{\numConsModes}$ which seeks to construct an improved trial space with which to represent the state vector computed from the FOM solution $\consVec$, given a fixed latent state $\consVecCoef$. This can be formalized at the $\timeIdx$th time step as
%
\begin{eqnarray}
	\consVec^{\timeIdx} = \consVecCent + \consScale \left[\consTrial^{\timeIdx} + \left[\consTrialUpdate\right]^{\timeIdx} \right] \consVecCoef^{\timeIdx},
\end{eqnarray}
%
where the FOM solution $\consVec^{\timeIdx}$ is computed in tandem with the low-dimensional ROM solution $\consVecCoef^{\timeIdx}$. Rearranging terms arrives at
%
\begin{eqnarray}
	\left[\consTrialUpdate\right]^{\timeIdx} \consVecCoef^{\timeIdx} = \consScaleInv \left[\consVec^{\timeIdx} - \consVecCent\right] - \consTrial^{\timeIdx} \consVecCoef^{\timeIdx}.
\end{eqnarray}
%
Obviously, this system of equations is underdetermined, attempting to solve for $\numDOF \times \numConsModes$ variables given $\numDOF$ equations. As such, there is no unique solution for $\left[\consTrialUpdate\right]^{\timeIdx}$. One solution suggested in~\cite{Huang2022a} is given as
%
\begin{equation}
	[\consTrialUpdate]^{\timeIdx} = \frac{\left[\consScaleInv\left[\consVec^{\timeIdx} - \consVecCent\right] - \consTrial^{\timeIdx} \consVecCoef^{\timeIdx}\right]\left[\consVecCoef^{\timeIdx}\right]^\top}{\left\Vert \consVecCoef^{\timeIdx} \right\Vert^2_2}
\end{equation}
%
Note that this approach is not strictly feasible given that the solution of the FOM equations $\consVec^{\timeIdx}$ must be computed in order to evaluate the trial basis update, thus defeating the purpose of the PROM. However, the usefulness of this approach becomes apparent when hyper-reduction is applied.

{\color{red}I have no idea where to go from here, the GEMS implementation does not match what was shared in the paper draft}

\begin{algorithm}
	\caption{One-step basis adaptation algorithm}\label{alg:oneStep}
	\begin{algorithmic}
		\STATE temp
	\end{algorithmic}
\end{algorithm}



% \begin{algorithm}
%     \caption{Basis and sampling adaptation}\label{alg:adapt}
%     \begin{algorithmic}

%     \STATE \texttt{init} $\leftarrow$ \texttt{istart} + \texttt{initWindow} - 1
%     \FOR{$\timeIdx$ = \texttt{istart} to \texttt{iend}}
%         \STATE $\consVecCoef^{\timeIdx} = \rhsApproxFunc{\consVecCoef^{\timeIdx-1}}$
%         \STATE $\consVecRom^{\timeIdx}[\sampVec^{\timeIdx-1}] = \consTrial^{\timeIdx-1}[\sampVec^{\timeIdx-1}, \; :] \consVecCoef^{\timeIdx}$
%         \IF{mod($\timeIdx$ - \texttt{init}, \texttt{basisInterval}) == 0 \AND $\timeIdx \ge$ \texttt{init}}
%             \STATE $\consVec^{\timeIdx}[\sampVec^{\timeIdx-1}] = \rhsFunc{\consVecRom^{\timeIdx - 1}[\sampVec^{\timeIdx-1}]}$
%             \STATE $\consTrial^{\timeIdx}$ = \texttt{OneStepAdapt}($\consVecCoef^{\timeIdx}, \; \consVecRom^{\timeIdx}[\sampVec^{\timeIdx-1}], \; \consVec^{\timeIdx}[\sampVec^{\timeIdx-1}]$)
%         \ELSE
%             \STATE $\consTrial^{\timeIdx} \leftarrow \consTrial^{\timeIdx - 1}$
%         \ENDIF
%         \IF{mod($n$ - \texttt{init}, \texttt{sampInterval}) == 0 \AND $\timeIdx \ge$ \texttt{init}}
%             \STATE $\consVec^{\timeIdx} = \rhsFunc{\consVecRom^{\timeIdx-1}}$
%             \STATE $\resVec^{\timeIdx} = \consVec^{\timeIdx} - \consTrial^{\timeIdx} [\consTrial^{\timeIdx}[\sampVec^{\timeIdx-1}, \; :]]^+ \consVec^{\timeIdx}[\sampVec^{\timeIdx - 1}]$
%             \STATE $\sampVec^{\timeIdx} = $ \texttt{argsort}$(\resVec^{\timeIdx})$[:\texttt{numSamps}]
%             % compute full-order residual
%             % calculate interpolation error
%             % argsort error
%             % pick samples
%         \ELSE
%             \STATE $\sampVec^{\timeIdx} \leftarrow \sampVec^{\timeIdx-1}$
%         \ENDIF
%     \ENDFOR

%     \end{algorithmic}
% \end{algorithm}
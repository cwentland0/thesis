So far, we have pointedly ignored the fact that the Galerkin (Eq.~\ref{eq:galerkinROMODE}), LSPG (Eq.~\ref{eq:lspgLSLin}), and MP-LSVT (Eq.~\ref{eq:mplsvtLSLin}) ROMs \textit{will not generate computational cost savings} for large-scale non-linear systems. This is largely due to the fact that evaluating the non-linear terms $\rhsFunc{\cdot}$ arising from fluxes, source terms, body forces, and boundary conditions still scales with the full-order dimension $\numDOF$. Linear time-invariant systems do not suffer from this issue; given a system of the form $\text{d}\consVec/\text{d}\timeVar - \dummyMat \consVec = \zeroVec$, $\dummyMat \inRTwo{\numDOF}{\numDOF}$, the resulting Galerkin ROM (assuming $\consScale = \resScale = \identMat$) takes the form
%
\begin{equation}
    \ode{\consVecCoef}{\timeVar} = \consTrial^\top \dummyMat \consTrial \consVecCoef.
\end{equation}
%
The matrix $\consTrial^\top \dummyMat \consTrial \consVecCoef \inRTwo{\numConsModes}{\numConsModes}$ can be precomputed in the \textit{offline} stage before evaluating the ROM in the \textit{online} stage. Such a precomputation is impossible for general non-linear systems. The low-dimensional state must first be \textit{lifted} to the full-dimensional state (via $\decoderFunc{\consVecCoef}$) to evaluate the non-linear terms $\rhsFunc{\cdot}$. In the case of Galerkin projection, this term must then be projected onto the tangent trial space before integrating the low-dimensional ODE in time. In the case of LSPG and MP-LSVT ROMs solved via the normal equations, the time-variant test basis must be computed from the residual Jacobian. Despite the fact that the resulting low-dimensional system may be less expensive to temporally integrate, these additional operations often outweigh any cost savings. Particularly for complex multi-physics systems, the evaluation of $\rhsFunc{\cdot}$ accounts for the vast majority of the solver cost, and failing to reduce this cost often fails to reduce the ROM cost below that of the FOM.

To achieve the intended goal of significantly reducing the cost of evaluating the model, we must eliminate the ROM's dependence on the full-order dimension $\numDOF$. Techniques which seek this goal are generally referred to as \textit{hyper-reduction} methods. Most prevalent and mature are so-called \textit{sparse sampling} methods, which evaluate the governing equations or non-linear terms at a small number of carefully selected degrees of freedom. For this reason, throughout the remainder of this thesis we will refer to non-hyper-reduced ROMs as \textit{unsampled ROMs}.

Work in this thesis focuses exclusively on linear sparse-sampling methods for linear subspace ROMs, including missing point estimation~\cite{Astrid2004}, collocation~\cite{Bos2004}, and methods of the \textit{approximate-then-project} type, namely the discrete empirical interpolation method~\cite{Chaturantabut2010} and gappy proper orthogonal decomposition~\cite{Everson1995}. Work on \textit{project-then-approximate} approaches, such as cubature methods~\cite{An2008,Hernandez2017} and the energy-conserving sampling and weighting method~\cite{Farhat2014}, show promise in enhancing the accuracy of hyper-reduced ROMs. To date, however, these methods have been analyzed almost exclusively for finite-element structural dynamics models. Their application to projection-based ROMs of hyperbolic fluid flow systems is still in its early stages~\cite{Grimberg2020Hyper}, and is not analyzed here. Neural network approaches to hyper-reduction~\cite{nnHyperRed} and hyper-reduction for non-linear manifold ROMs~\cite{Kim2022} have been proposed, but are also not analyzed here.

Before proceeding, we define a \textit{sampling operator} $\sampMat \defEq [\canonVec_{\dummyIdx_1}, \; \canonVec_{\dummyIdx_2}, \; \hdots, \; \canonVec_{\dummyIdx_{\numSamps}} ]^\top \inRTwo{\numSamps}{\numDOF}$, which is composed of $\numSamps$ unique canonical unit vectors $\canonVec_{\dummyIdx} \inROne{\numDOF}$. For example, given $\numDOF = 5$, $\numSamps = 3$, $\sampMat = [\canonVec_1, \; \canonVec_3, \; \canonVec_4]$,
and a vector $\dummyVec = [\dummyVecVar_1, \; \hdots, \; \dummyVecVar_5]^\top$, the sampling operation $\sampMat \dummyVec$ is computed as
%
\begin{equation}
	\sampMat \dummyVec =
	\begin{bmatrix}
		1 & 0 & 0 & 0 & 0 \\
		0 & 0 & 1 & 0 & 0 \\
		0 & 0 & 0 & 1 & 0 \\
	\end{bmatrix}
	\begin{bmatrix}
		\dummyVecVar_1 \\ \dummyVecVar_2 \\ \dummyVecVar_3 \\ \dummyVecVar_4 \\ \dummyVecVar_5
	\end{bmatrix} =
	\begin{bmatrix}
		\dummyVecVar_1 \\ \dummyVecVar_3 \\ \dummyVecVar_4
	\end{bmatrix}
\end{equation}
%
Such sampling operations are central to each sparse-sampling method described here, and how the sampling indices are selected may have a drastic effect on the accuracy and robustness of the hyper-reduced ROM. Several algorithms for selecting sampling indices are described later in Section~\ref{subsec:sampleSelect}. We now begin by discussing missing point estimation for Galerkin ROMs and collocation for LSPG and MP-LSVT ROMs, followed by gappy POD for each PROM method.